{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32d6abfc-868a-46d8-98d3-3ea84445e872",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib widget\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch import nn\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "from oxfordiit_pet_dataset import get_datasets\n",
    "from torchvision import datapoints\n",
    "from torchvision.transforms import v2\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from datetime import datetime\n",
    "import os\n",
    "from time import sleep\n",
    "seed = 42\n",
    "torch.manual_seed(seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b92c0ce",
   "metadata": {},
   "source": [
    "In this experiment I wanted to combine the creation of a Transformer from scratch with a computer vision application since that's more my expertise. I used the following blog posts as references.  \n",
    "For the Transformer from scratch\n",
    "https://peterbloem.nl/blog/transformers  \n",
    "For the Vision Transformer  \n",
    "https://towardsdatascience.com/efficient-image-segmentation-using-pytorch-part-4-6c86da083432"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fde5bdb",
   "metadata": {},
   "source": [
    "Playing around to get a fast way to get the patches in correct order (left to right, bottom to top) using views.\n",
    "Order at the end should not matter as long as it is consistent. The positional embedding will learn or encode the order (like right to left, top to bottom for instance instead of the one above)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4151b0d5-c173-40aa-bf96-143f64a5f403",
   "metadata": {},
   "outputs": [],
   "source": [
    "sz0 = 10\n",
    "sz1 = 10\n",
    "a = torch.zeros(3,2,sz0,sz1)\n",
    "cnt = 0\n",
    "stride = 5\n",
    "new_dim0 = sz0//stride\n",
    "new_dim1 = sz1//stride\n",
    "for k in range(a.shape[1]):\n",
    "    for i in range(new_dim0):\n",
    "        for j in range(new_dim1):\n",
    "            a[:,k,i*stride:(i+1)*stride,j*stride:(j+1)*stride] = cnt\n",
    "            cnt += 1\n",
    "print(a[0])\n",
    "print(a[0,0].view(new_dim0,stride,sz1))\n",
    "print(a[0,0].view(new_dim0,stride,new_dim1,stride).permute(0,2,1,3))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f8756de",
   "metadata": {},
   "source": [
    "This function returns the patches from an image. The number of patches will be equivalent of the time dimension for words in a transformer architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d018280",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_patches(inp,patch_size):\n",
    "    #Assume images are square like b x c x h x w and divisible by patch_size \n",
    "    ishape = inp.shape\n",
    "    assert ishape[-1] == ishape[-2], 'Images must be square'\n",
    "    assert (ishape[-1] % patch_size) == 0, 'Width and lenght must be divisibple by patch_size'\n",
    "    new_dim = ishape[-2]//patch_size\n",
    "    #reshape the last two dimensions, then permute (0,1,2,4,3,5) so that the last 2 dims are patch_size x patch_size.\n",
    "    #permute again to put the channel next to height and width (0,2,3,1,4,5)\n",
    "    ret =  inp.view(*ishape[:2],new_dim,patch_size,new_dim,patch_size).permute(0,1,2,4,3,5).permute((0,2,3,1,4,5))\n",
    "    #ret.shape = b x h/patch_size x w/patch_size x c x patch_size x patch_size\n",
    "    rshape = ret.shape\n",
    "    #reshape into b x t x l with t = \"timesteps\" i.e. number of patches h/patch_size x w/patch_size and l = c x patch_size x patch_size\n",
    "    #kind of the emb_sizeedding size \n",
    "    return ret.reshape((rshape[0],rshape[1]*rshape[2],-1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe92f300",
   "metadata": {},
   "source": [
    "Here are some tests to make sure that what is returned in the forward method of the VisTransformer undoes the get_patches transforms to make sure the right pixels are attended. First generate the different steps from the *get_paches* method and then undo each in reverse order to get back the original input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26c42f3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "inp = torch.tensor(np.arange(2*128*128*3).reshape([2,3,128,128]).astype(np.float32))\n",
    "\n",
    "out = get_patches(inp,16)\n",
    "new_dim = 8\n",
    "patch_size = 16\n",
    "ishape = inp.shape\n",
    "#save each stage of get_patches for later comparison\n",
    "inp1 = inp.view(*ishape[:2],new_dim,patch_size,new_dim,patch_size)\n",
    "inp1_1 = inp1.permute((0,1,2,4,3,5))\n",
    "inp2 = inp1_1.permute((0,2,3,1,4,5))\n",
    "rshape = inp2.shape\n",
    "inp3 = inp2.reshape([rshape[0],rshape[1]*rshape[2],-1])\n",
    "#compare with get_patches\n",
    "np.where(inp3.numpy() - out.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42d16772",
   "metadata": {},
   "outputs": [],
   "source": [
    "b,t,nc,ps,ms=2,64,3,16,128\n",
    "st = int(t**.5)\n",
    "#Undo one transform at the time and make sure they agree\n",
    "print('inp3',np.where(inp3.reshape(b,st,st,nc,ps,ps).numpy() - inp2.numpy()))\n",
    "print('inp2',np.where(inp3.reshape(b,st,st,nc,ps,ps).permute(0,3,1,2,4,5).numpy() - inp1_1.numpy()))\n",
    "print('inp1_1',np.where(inp3.reshape(b,st,st,nc,ps,ps).permute(0,3,1,2,4,5).permute(0,1,2,4,3,5).numpy() - inp1.numpy()))\n",
    "print('inp1',np.where(inp3.reshape(b,st,st,nc,ps,ps).permute(0,3,1,2,4,5).permute(0,1,2,4,3,5).reshape((b,nc,ms,ms)).numpy() - inp.numpy()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6693a7e4",
   "metadata": {},
   "source": [
    "Create all the elements of a Transformer, from the self-attention to the transformer block. The Vision Transfomer is very similar to the encoder of the original Transformer architecture but uses *GELU* activation instead of *ReLU*. I also needed to change the order of some layers (dropout and normlayer) in the TransformerBlock compared with the \"Transfoemr from scratch\" reference since the model was not learning.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5af20bde-30f3-44dd-ad89-a36f8b2579ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEmbeddingLearn():\n",
    "    def __init__(self,max_seq_len,msize):\n",
    "        self.max_seq_len = max_seq_len\n",
    "        self.msize = msize\n",
    "        self.create_pe()\n",
    "    \n",
    "    def create_pe(self):\n",
    "            self.pe = nn.Embedding(self.max_seq_len,self.msize)\n",
    "       \n",
    "    def __call__(self,sel):\n",
    "        return self.pe(sel)\n",
    "\n",
    "class PositionalEmbeddingTrig():\n",
    "    def __init__(self,max_seq_len,msize,base=10000):\n",
    "        self.max_seq_len = max_seq_len\n",
    "        self.msize = msize\n",
    "        self.base = base\n",
    "        self.create_pe()\n",
    "    \n",
    "    def create_pe(self):\n",
    "        xi = np.arange(0,self.msize,2)\n",
    "        x = np.zeros((self.max_seq_len,self.msize),np.int32)\n",
    "        x[:,::2] = np.tile(xi,[self.max_seq_len,1])\n",
    "        x[:,1::2] = np.tile(xi,[self.max_seq_len,1])\n",
    "        pos = np.tile(np.arange(self.max_seq_len).reshape([-1,1]),[1,self.msize])\n",
    "        self.pe = np.zeros_like(x,np.float32)\n",
    "        self.pe[:,::2] = np.sin(pos[:,::2]/self.base**(x[:,::2]/self.msize))\n",
    "        self.pe[:,1::2] = np.cos(pos[:,1::2]/self.base**(x[:,1::2]/self.msize))\n",
    "        self.pe = torch.from_numpy(self.pe)\n",
    "\n",
    "    def __call__(self,sel):\n",
    "        return self.pe[sel]\n",
    "\n",
    "def get_embedding(etype,max_seq_len,msize,base=10000):\n",
    "    if etype == 'trig':\n",
    "        return PositionalEmbeddingTrig(max_seq_len,msize,base)\n",
    "    elif etype == 'learn':\n",
    "        return PositionalEmbeddingLearn(max_seq_len,msize)\n",
    "    else:\n",
    "        raise ValueError(f'Unknown embedding type {etype}')\n",
    "\n",
    "class SelfAttention(nn.Module):\n",
    "    \n",
    "    def __init__(self,emb_size,num_heads=8,mask=False):\n",
    "        super().__init__()\n",
    "        assert emb_size % num_heads == 0, f'Embedding size {emb_size} must be divisible by number of heads {num_heads}'\n",
    "\n",
    "        self.num_heads = num_heads\n",
    "        self.emb_size = emb_size\n",
    "        self.mask = mask\n",
    "        self.get_keys = nn.Linear(emb_size,emb_size,bias=False)\n",
    "        self.get_queries = nn.Linear(emb_size,emb_size,bias=False)\n",
    "        self.get_values = nn.Linear(emb_size,emb_size,bias=False)\n",
    "\n",
    "        self.linear = nn.Linear(emb_size,emb_size)\n",
    "    \n",
    "    def apply_mask(self,x,mask_val=np.inf):\n",
    "        h,w = x.size(-2),x.size(-1)\n",
    "        indices = torch.triu_indices(h,w,offset=1)\n",
    "        x[...,indices[0],indices[1]] = mask_val\n",
    "        return x\n",
    "\n",
    "    def forward(self,x):\n",
    "        b,t,e = x.size()\n",
    "        h = self.num_heads\n",
    "        s = e // h\n",
    "\n",
    "        keys = self.get_keys(x)\n",
    "        queries = self.get_queries(x)\n",
    "        values = self.get_values(x)\n",
    "\n",
    "        keys = keys.view(b,t,h,s)\n",
    "        queries = queries.view(b,t,h,s)\n",
    "        values = values.view(b,t,h,s)\n",
    "\n",
    "        keys = keys.transpose(1,2).contiguous().view(b*h,t,s)\n",
    "        queries = queries.transpose(1,2).contiguous().view(b*h,t,s)\n",
    "        values = values.transpose(1,2).contiguous().view(b*h,t,s)\n",
    "\n",
    "        dot_qk = torch.bmm(queries,keys.transpose(1,2))/e**(1/2)\n",
    "\n",
    "        if self.mask:\n",
    "            dot_qk = self.apply_mask(dot_qk)\n",
    "        dot_qk = F.softmax(dot_qk,dim=2)\n",
    "        out = torch.bmm(dot_qk,values).view(b,h,t,s)\n",
    "        out = out.transpose(1,2).contiguous().view(b,t,h*s)\n",
    "        return self.linear(out)\n",
    "\n",
    "class TransformerBlock(nn.Module):\n",
    "    def __init__(self,emb_size,num_heads=8,hidden_mult=4,dropout=0.0,mask=False):\n",
    "        super().__init__()\n",
    "        self.attention = SelfAttention(emb_size,num_heads,mask)\n",
    "        #self.attention = nn.MultiheadAttention(emb_size,num_heads,dropout)\n",
    "        self.norm1 = nn.LayerNorm(emb_size)\n",
    "        self.do1 = nn.Dropout(dropout)\n",
    "        self.ff = nn.Sequential(\n",
    "            nn.Linear(emb_size,hidden_mult*emb_size),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(hidden_mult*emb_size,emb_size)\n",
    "            )\n",
    "        self.norm2 = nn.LayerNorm(emb_size)\n",
    "        self.do2 = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self,x):\n",
    "        xn = self.norm1(x)\n",
    "        x_att = x + self.do1(self.attention(xn))\n",
    "        ff = self.ff(self.norm2(x_att))\n",
    "        x = self.do2(ff)\n",
    "        x = x_att + x\n",
    "        return x\n",
    "\n",
    "class VisTransformer(nn.Module):\n",
    "    def __init__(self,args):\n",
    "        super().__init__()\n",
    "        etype = args.get('etype','trig')\n",
    "        seq_len = args['seq_len']\n",
    "        emb_size = args['emb_size']\n",
    "        base = args.get('base',10000)\n",
    "        num_heads = args.get('num_heads',4)\n",
    "        hidden_mult = args.get('hidden_mult',4)\n",
    "        dropout = args.get('dropout',0)\n",
    "        self.device = args.get('device','cpu')\n",
    "        mask = args.get('mask',False)\n",
    "        depth = args.get('depth',4)\n",
    "        self.patch_size = args['patch_size']\n",
    "        self.num_classes = args.get('num_classes',3)\n",
    "        ch_in = args.get('ch_in',3)\n",
    "        in_size = self.patch_size*self.patch_size*ch_in\n",
    "\n",
    "        #NOTE: seq_len is the number of patches N (which is width x length)/(patch_size**2) \n",
    "        #and width = length\n",
    "        self.img_size = int(self.patch_size*seq_len**.5)\n",
    "        self.pos_emb = get_embedding(etype,seq_len,emb_size,base)\n",
    "        self.token_emb = nn.Linear(in_size,emb_size)\n",
    "        self.batch_norm = nn.BatchNorm2d(3)\n",
    "        tblocks = []\n",
    "        for _ in range(depth):\n",
    "            tblocks.append(TransformerBlock(emb_size,num_heads,hidden_mult,dropout,mask))\n",
    "            #tblocks.append(SelfAttentionEncoderBlock(emb_size,num_heads,dropout=dropout))\n",
    "        self.tblocks = nn.Sequential(*tblocks)\n",
    "        self.out_layer = nn.Linear(emb_size,self.num_classes*self.patch_size*self.patch_size)\n",
    "        self.do = nn.Dropout(dropout)\n",
    "        self.pos_indx = np.arange(seq_len)\n",
    "    \n",
    "    \n",
    "    def forward(self,x):\n",
    "        x = self.batch_norm(x)\n",
    "        x = get_patches(x,self.patch_size)\n",
    "        tokens = self.token_emb(x)\n",
    "        b,t,e = tokens.size()\n",
    "        st = int(t**.5)\n",
    "        positions = self.pos_emb(self.pos_indx)[None,:,:].expand(b,t,e).to(self.device)\n",
    "        x = positions + tokens\n",
    "        #x = tokens\n",
    "        x = self.do(x)\n",
    "        x = self.tblocks(x)\n",
    "        x = self.out_layer(x)\n",
    "        # x.shape = (b,t,self.num_classes*self.patch_size*self.patch_size)\n",
    "        return x.reshape(b,st,st,self.num_classes,self.patch_size,self.patch_size).permute(0,3,1,2,4,5).permute(0,1,2,4,3,5).reshape((b,self.num_classes,self.img_size,self.img_size))\n",
    "\n",
    "def train_one_epoch(epoch_index,model,loss_fn,training_loader,optimizer, tb_writer,print_every=100,device='cuda'):\n",
    "    \"\"\"\n",
    "    Modified version from Pytorch web site. Passed arguments explicitly. \n",
    "    \"\"\"\n",
    "    running_loss = 0.\n",
    "    last_loss = 0.\n",
    "    #print info only print_every is > 0.\n",
    "    do_print = True if print_every > 0 else False\n",
    "    print_every = abs(print_every)\n",
    "\n",
    "    # Here, we use enumerate(training_loader) instead of\n",
    "    # iter(training_loader) so that we can track the batch\n",
    "    # index and do some intra-epoch reporting\n",
    "    for i, data in enumerate(training_loader):\n",
    "        # Every data instance is an input + label pair\n",
    "        inputs, labels = data\n",
    "        #inputs = get_patches(inputs_big,patch_size)\n",
    "\n",
    "        # Zero your gradients for every batch!\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Make predictions for this batch\n",
    "        outputs = model(inputs.to(device))\n",
    "        # Compute the loss and its gradients        \n",
    "        loss = loss_fn(outputs, torch.squeeze(labels,dim=1).to(device))\n",
    "        loss.backward()\n",
    "\n",
    "        # Adjust learning weights\n",
    "        optimizer.step()\n",
    "\n",
    "        # Gather data and report\n",
    "        running_loss += loss.item()\n",
    "        if i % print_every == print_every - 1:\n",
    "            last_loss = running_loss / print_every # loss per batch\n",
    "            if do_print:\n",
    "                print(f'batch {i+1} loss: {last_loss}')\n",
    "            tb_x = epoch_index * len(training_loader) + i + 1\n",
    "            tb_writer.add_scalar('Loss/train', last_loss, tb_x)\n",
    "            running_loss = 0.\n",
    "\n",
    "    return last_loss\n",
    "\n",
    "def do_eval(model,loss_fn,validation_loader,device='cuda'):\n",
    "    running_vloss = 0.0\n",
    "    # Set the model to evaluation mode, disabling dropout and using population\n",
    "    # statistics for batch normalization.\n",
    "    model.eval()\n",
    "    nplt = 2\n",
    "    # Disable gradient computation and reduce memory consumption.\n",
    "    with torch.no_grad():\n",
    "        for i, vdata in enumerate(validation_loader):\n",
    "            vinputs, vlabels = vdata\n",
    "            #vinputs = get_patches(vinputs_big,patch_size)\n",
    "            voutputs = model(vinputs.to(device))\n",
    "            vloss = loss_fn(voutputs, torch.squeeze(vlabels,dim=1).to(device))\n",
    "            running_vloss += vloss\n",
    "    return running_vloss,i ,vinputs,vlabels,voutputs   \n",
    "#Test get_patches with matrix a prev cell\n",
    "#get_patches(a,5)\n",
    "#Test trigonometric embedding\n",
    "# emb_gen = get_embedding('trig',200,500)\n",
    "# all_emb = emb_gen(np.arange(200))\n",
    "# plt.figure()\n",
    "# plt.imshow(all_emb)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59f9c44a",
   "metadata": {},
   "source": [
    "Quick test to check that the model spits out  the correct output size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73ab5fe6-aa1b-4aee-8d64-0f185875dfaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "patch_size = 16\n",
    "img_size = 128\n",
    "seq_len = img_size**2//patch_size**2\n",
    "emb_size = 768\n",
    "num_heads = 8\n",
    "dropout = 0.1\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "depth = 12\n",
    "ch_in = 3\n",
    "args = {'etype':'trig',\n",
    "        'seq_len':seq_len,\n",
    "        'emb_size':emb_size,\n",
    "        'num_heads':num_heads,\n",
    "        'dropout':dropout,\n",
    "        'device':device,\n",
    "        'depth':depth,\n",
    "        'patch_size':patch_size,\n",
    "        'ch_in':ch_in\n",
    "       }\n",
    "model = VisTransformer(args).to(device)\n",
    "#Quick test. Expected \n",
    "inp = torch.randn((2,3,128,128)).to(device)\n",
    "print(model(inp).shape)\n",
    "#print(np.where(model(inp).detach().cpu().numpy() - inp.detach().cpu().numpy()))\n",
    "\n",
    "del model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38d40bff",
   "metadata": {},
   "source": [
    "As for my other Unet project  I create randomized datasets for training and validation. Use a set of image transforms for data augmentation. In this examples I used a BatchNorm2D as first layer given that the batch_size is big enough to noramlized the imaged. Use AdamW optimizer with Cyclic leraning rate scheduler. For loss I use pixel-wise CrossEntropyLoss. Given that the labels (foreground,background,border) are imbalanced one could use the *weight_loss* to favor one more that the others."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bea78522-a0a9-4863-9935-ca24b059d91c",
   "metadata": {},
   "outputs": [],
   "source": [
    "resize_to = 256\n",
    "crop_to = img_size\n",
    "transforms = v2.Compose(\n",
    "[\n",
    "    v2.PILToTensor(),\n",
    "    v2.ToDtype(torch.float32),\n",
    "    v2.Resize(resize_to),#resize to a square image regardless of aspect ratio\n",
    "    v2.RandomCrop(crop_to,pad_if_needed=True,padding_mode='reflect'),#get a smaller random crop\n",
    "    v2.RandomHorizontalFlip(),\n",
    "    v2.RandomVerticalFlip()\n",
    "]\n",
    ")\n",
    "#If forget to restart kernel do some manual cleanup\n",
    "torch.cuda.empty_cache()\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "batch_size = 64\n",
    "root = '/home/giangi/Workspace/Data/oxford-iiit-pet'\n",
    "train_ds,val_ds = get_datasets(root,.8,transforms=transforms)\n",
    "train_dl = DataLoader(train_ds,batch_size=batch_size,shuffle=True)\n",
    "val_dl = DataLoader(val_ds,batch_size=batch_size,shuffle=True)\n",
    "model = VisTransformer(args).to(device)\n",
    "lr = 0.0004\n",
    "base_lr = 0.0004\n",
    "max_lr = 0.001\n",
    "step_size_up = 10\n",
    "gamma_sch = .9\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=lr)\n",
    "scheduler = torch.optim.lr_scheduler.CyclicLR(optimizer, base_lr=base_lr, max_lr=max_lr,step_size_up=step_size_up,mode=\"exp_range\",gamma=gamma_sch,cycle_momentum=False)\n",
    "#can play with weight_loss to give more weight to most mislabeled\n",
    "weight_loss = torch.ones(3).to(device)\n",
    "loss_fn = nn.CrossEntropyLoss(weight_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "594d9c54",
   "metadata": {},
   "source": [
    "Train the model for 100 epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1343cb57-5a5f-44fe-8c92-0a0e3b5b981f",
   "metadata": {},
   "outputs": [],
   "source": [
    "timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "writer = SummaryWriter(f'runs/vtrans_trainer_{timestamp}')\n",
    "epoch_number = 0\n",
    "print_every = -10 #negative if don't want to print but just save'\n",
    "EPOCHS = 100\n",
    "model_dir = 'models'\n",
    "best_vloss = 1e10\n",
    "# fig = plt.figure()\n",
    "# ax = fig.gca()\n",
    "# fig.show()\n",
    "nplt = 2\n",
    "do_plot = True\n",
    "for epoch in range(EPOCHS):\n",
    "    print(f'EPOCH {epoch_number + 1}:')\n",
    "\n",
    "    # Make sure gradient tracking is on, and do a pass over the data\n",
    "    model.train(True)\n",
    "    avg_loss = train_one_epoch(epoch,model,loss_fn,train_dl,optimizer,writer,print_every,device)\n",
    "\n",
    "    running_vloss,i,vin,vlab,vout = do_eval(model,loss_fn,val_dl,device)\n",
    "    \n",
    "    avg_vloss = running_vloss / (i + 1)\n",
    "    print(f'LOSS train {avg_loss} valid {avg_vloss}')\n",
    "\n",
    "    # Log the running loss averaged per batch\n",
    "    # for both training and validation\n",
    "    writer.add_scalars('Training vs. Validation Loss',\n",
    "                    { 'Training' : avg_loss, 'Validation' : avg_vloss },\n",
    "                    epoch_number + 1)\n",
    "    writer.flush()\n",
    "\n",
    "    # Track best performance, and save the model's state\n",
    "    if avg_vloss < best_vloss:\n",
    "        best_vloss = avg_vloss\n",
    "        model_path = os.path.join(model_dir,f'transf_model_{timestamp}_{epoch_number}')\n",
    "        torch.save(model.state_dict(), model_path)\n",
    "\n",
    "    epoch_number += 1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5b3bcef",
   "metadata": {},
   "source": [
    "I really liked the plotting from [this](https://towardsdatascience.com/efficient-image-segmentation-using-pytorch-part-4-6c86da083432) post that I mentioned at the beginning so I coded something similar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d48c50b0-9c29-4b25-9543-84a15960409d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Do some plotting to compare\n",
    "import torchvision\n",
    "nrow = 2\n",
    "ncol = 6\n",
    "nplt = nrow*ncol\n",
    "plt.close('all')\n",
    "with torch.no_grad():\n",
    "    for i, vdata in enumerate(val_dl):\n",
    "        vin, vlab = vdata\n",
    "        #vinputs = get_patches(vinputs_big,patch_size)\n",
    "        vout = torch.argmax(torch.softmax(model(vin.to(device))[:nplt],dim=1),dim=1).unsqueeze(1)\n",
    "        pvout = torchvision.utils.make_grid(vout,ncol).detach().cpu().numpy()\n",
    "        pvlab = torchvision.utils.make_grid(vlab[:nplt],ncol).detach().cpu().numpy()\n",
    "        pvin = torchvision.utils.make_grid(vin[:nplt],ncol).detach().cpu().numpy()\n",
    "        pvin = pvin\n",
    "        plt.figure(figsize=(10,12))\n",
    "        plt.subplot(3,1,1)\n",
    "        plt.imshow(pvin.transpose(1,2,0).astype(int))\n",
    "        plt.axis('off')\n",
    "        plt.title('Images')\n",
    "        plt.subplot(3,1,2)\n",
    "        #it creates 3 channels that are the same image. taks the first\n",
    "        plt.imshow(pvlab[0])\n",
    "        plt.axis('off')\n",
    "        plt.title('Labels')\n",
    "        plt.subplot(3,1,3)\n",
    "        #it creates 3 channels that are the same image. taks the first\n",
    "        plt.imshow(pvout[0])\n",
    "        plt.axis('off')\n",
    "        plt.title('Pred')\n",
    "        break\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2593a248",
   "metadata": {},
   "source": [
    "Reload the best model and compute predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3deddb3-7553-4dbf-b3d3-ef1d0959eb8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get all the pridictions and labels to check accuracy\n",
    "device = 'cuda'\n",
    "model = VisTransformer(args).to(device)\n",
    "model.load_state_dict(torch.load('models/transf_model_20240228_150916_88'))\n",
    "tot_lab = []\n",
    "tot_pred = []\n",
    "to_plt = []\n",
    "model.eval()\n",
    "# Disable gradient computation and reduce memory consumption.\n",
    "with torch.no_grad():\n",
    "    for i, vdata in enumerate(val_dl):\n",
    "        vinputs, vlabels = vdata\n",
    "        voutputs = model(vinputs.to(device))\n",
    "        vout = np.argmax(voutputs.detach().to('cpu').numpy().transpose([0,2,3,1]),axis=3)\n",
    "        vlab = np.squeeze(vlabels.detach().to('cpu').numpy().transpose([0,2,3,1]))\n",
    "        if i == len(val_dl) - 2:\n",
    "            to_plt = [vinputs, vlab.copy(),vout.copy()]\n",
    "        tot_lab.append(vlab) \n",
    "        tot_pred.append(vout)\n",
    "        \n",
    "tot_lab = np.concatenate(tot_lab)\n",
    "tot_pred = np.concatenate(tot_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb0ab43d",
   "metadata": {},
   "source": [
    "Compute confusion matrix manually. Normalize w.r.t. true values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58e60581-d7bb-4ab8-a7ee-f3010ed2f5e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "cm = np.zeros((3,3))\n",
    "for i in range(3):\n",
    "    for j in range(3):\n",
    "        cm[i,j] += np.where((tot_lab == i)*(tot_pred == j))[0].size\n",
    "cm/cm.sum(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a155737b",
   "metadata": {},
   "source": [
    "Plot confusion matrix using sklearn normalize w.r.t. true values. Should match the manual one. As for the Unet experiment, the model reachs a good accurary (technically recall since we normalized by the true label) in determining the foreground (label 0) and the background (label 1). It is not so good in determining the edges between foreground and background (label 2) so one could try to change the weights given to each label in the loss function to see if things improve. Also the edge could be a bit ambiguous the way it is defined so that could also be a source of error. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce21883b-e910-406c-aa5f-5ed02aa2730f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import ConfusionMatrixDisplay\n",
    "ConfusionMatrixDisplay.from_predictions(tot_lab.reshape([-1]),tot_pred.reshape([-1]),normalize='true')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "c55c4586e5d8367157262f4649453ccda9ce6ed7633b9e450f38003cd2d34f54"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
